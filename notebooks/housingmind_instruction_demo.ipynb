# HousingMind: Instruction Dataset Preview & Sample Fine-Tuning Prep

## 1. Introduction

"""
This notebook demonstrates how to load, inspect, and prepare HousingMind instruction data
for use in training or fine-tuning a language model for housing policy applications.
"""

## 2. Imports
import json
import pandas as pd
from pathlib import Path

## 3. Load Instruction Data (JSONL)

instruction_path = Path("../instruction_data/housingmind_instructions.jsonl")

# Read JSONL into a DataFrame
records = []
with open(instruction_path, 'r') as file:
    for line in file:
        records.append(json.loads(line.strip()))

df = pd.DataFrame(records)

# Preview
df.head()

## 4. Data Schema Check
print("Total Records:", len(df))
print("Columns:", df.columns.tolist())

# Check for nulls or formatting issues
print(df.isnull().sum())

## 5. Sample Prompt-Response Pair
sample = df.sample(1).iloc[0]
print("ðŸ§  Prompt:", sample["prompt"])
print("âœ… Response:", sample["response"])

## 6. Prepare Data for Training (e.g., LoRA, QLoRA)
# This step will vary depending on your model and framework.
# Example: format for Hugging Face datasets or Alpaca-style fine-tuning.

def to_alpaca_format(df):
    return [
        {
            "instruction": row["prompt"],
            "input": "",
            "output": row["response"]
        }
        for _, row in df.iterrows()
    ]

alpaca_data = to_alpaca_format(df)

# Save formatted output (optional)
with open("formatted_housingmind_alpaca.json", "w") as f:
    json.dump(alpaca_data, f, indent=2)

## 7. Next Steps
"""
- Upload to Hugging Face Datasets or integrate with LangChain / RAG.
- Tokenize and feed into a LoRA fine-tuner.
- Expand with additional examples from raw_documents or lookup_tables.
"""
